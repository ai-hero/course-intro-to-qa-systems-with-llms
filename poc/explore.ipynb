{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkNZ6hp6K8l6"
   },
   "source": [
    "# Retrieval Augmented Generation using LLMs on Proprietary Data \n",
    "\n",
    "In this tutorial, we'll create a proof of concept prototype that uses MLOps community data from our Slack channel and answer questions based on combined community knowledge.\n",
    "\n",
    "### IMPORTANT:\n",
    "Please use the virtualenv set up for you following the README in the parent project.\n",
    "It will install the dependencies needed for the entire repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTCDcPB9LMjx"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ENVIRONMENT VARIABLES\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=\"./.env\")  # Change this to your own .env file, if not using the venv\n",
    "assert os.getenv(\"OPENAI_API_KEY\") is not None, \"OPENAI_API_KEY not found in .env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1686710999797,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "iMTC5i-bMLwD"
   },
   "outputs": [],
   "source": [
    "# Since we have some long lines, let's make sure output is line wrapped\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def set_css():\n",
    "    display(\n",
    "        HTML(\n",
    "            \"\"\"\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "get_ipython().events.register(\"pre_run_cell\", set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYS1Vk-xNRBj"
   },
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "executionInfo": {
     "elapsed": 45507,
     "status": "ok",
     "timestamp": 1686711045297,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "Wxl__9HQGoML",
    "outputId": "864f6bc9-7f44-4a22-d395-60f45f551b84"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "from download_chats import download_chats_from_gdrive\n",
    "\n",
    "download_chats_from_gdrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff4PODEINaXH"
   },
   "source": [
    "### Raw Messages & Embeddings\n",
    "\n",
    "In most real-world applications, data that you're building on top of is never clean. While we've done some preprocessing on our Slack raw data from the community, we're keeping the data as original as possible so that you can explore some of the challeneges in preparing the data for retrieval augmnented generation.\n",
    "\n",
    "Let's load up the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1686711046444,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "Gb3WVLtRJpsN",
    "outputId": "f33c5507-9668-43cd-c7ae-aa5d0e48941b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__Source</th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Channel_Name</th>\n",
       "      <th>Message_Timestamp</th>\n",
       "      <th>Thread_Timstamp</th>\n",
       "      <th>Channel_ID</th>\n",
       "      <th>__Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>threads</td>\n",
       "      <td>U01T78HPG3H</td>\n",
       "      <td>computer-vision</td>\n",
       "      <td>2023-05-18 11:36:44.001949 UTC</td>\n",
       "      <td>2023-05-18 11:30:13.159979 UTC</td>\n",
       "      <td>C026ED0PZEZ</td>\n",
       "      <td>Both &lt;https://roboflow.github.io/supervision/q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>threads</td>\n",
       "      <td>U01T78HPG3H</td>\n",
       "      <td>computer-vision</td>\n",
       "      <td>2023-05-18 11:30:13.159979 UTC</td>\n",
       "      <td>2023-05-18 11:30:13.159979 UTC</td>\n",
       "      <td>C026ED0PZEZ</td>\n",
       "      <td>Would love a package for suggesting and then i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>threads</td>\n",
       "      <td>U04QRD69H8Q</td>\n",
       "      <td>computer-vision</td>\n",
       "      <td>2023-05-18 10:56:33.313369 UTC</td>\n",
       "      <td>2023-05-17 12:35:14.522419 UTC</td>\n",
       "      <td>C026ED0PZEZ</td>\n",
       "      <td>&lt;@U056Q4V4FFC&gt; how about this dataset (for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threads</td>\n",
       "      <td>U01J0NVNE1G</td>\n",
       "      <td>mlops-questions-answered</td>\n",
       "      <td>2023-05-18 05:21:04.346819 UTC</td>\n",
       "      <td>2023-05-18 01:08:57.948139 UTC</td>\n",
       "      <td>C015J2Y9RLM</td>\n",
       "      <td>I always oppose the counterargument, why do yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>threads</td>\n",
       "      <td>U01J0NVNE1G</td>\n",
       "      <td>mlops-questions-answered</td>\n",
       "      <td>2023-05-18 05:19:51.718379 UTC</td>\n",
       "      <td>2023-05-18 01:08:57.948139 UTC</td>\n",
       "      <td>C015J2Y9RLM</td>\n",
       "      <td>I find MLflow more convenient to use. Here are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  __Source      User_ID              Channel_Name  \\\n",
       "0  threads  U01T78HPG3H           computer-vision   \n",
       "1  threads  U01T78HPG3H           computer-vision   \n",
       "2  threads  U04QRD69H8Q           computer-vision   \n",
       "3  threads  U01J0NVNE1G  mlops-questions-answered   \n",
       "4  threads  U01J0NVNE1G  mlops-questions-answered   \n",
       "\n",
       "                Message_Timestamp                 Thread_Timstamp  \\\n",
       "0  2023-05-18 11:36:44.001949 UTC  2023-05-18 11:30:13.159979 UTC   \n",
       "1  2023-05-18 11:30:13.159979 UTC  2023-05-18 11:30:13.159979 UTC   \n",
       "2  2023-05-18 10:56:33.313369 UTC  2023-05-17 12:35:14.522419 UTC   \n",
       "3  2023-05-18 05:21:04.346819 UTC  2023-05-18 01:08:57.948139 UTC   \n",
       "4  2023-05-18 05:19:51.718379 UTC  2023-05-18 01:08:57.948139 UTC   \n",
       "\n",
       "    Channel_ID                                             __Text  \n",
       "0  C026ED0PZEZ  Both <https://roboflow.github.io/supervision/q...  \n",
       "1  C026ED0PZEZ  Would love a package for suggesting and then i...  \n",
       "2  C026ED0PZEZ  <@U056Q4V4FFC> how about this dataset (for the...  \n",
       "3  C015J2Y9RLM  I always oppose the counterargument, why do yo...  \n",
       "4  C015J2Y9RLM  I find MLflow more convenient to use. Here are...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Raw messages\n",
    "messages_df = pd.read_csv(\"./.content/chats/messages.csv\")\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see above that each row represents a chat message. Each message has the user id of the poster, the channel name in which it was posted, the message and thread timestamps, and the text of the message.\n",
    "\n",
    "Let's filter messages only on the mlops-questions-answered channel to create our Milo bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1686708556959,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "t2DI96m7OOjK",
    "outputId": "f1e183d7-3917-4ba0-cae1-e55f6b26368c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__Source</th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Channel_Name</th>\n",
       "      <th>Message_Timestamp</th>\n",
       "      <th>Thread_Timstamp</th>\n",
       "      <th>Channel_ID</th>\n",
       "      <th>__Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threads</td>\n",
       "      <td>U01J0NVNE1G</td>\n",
       "      <td>mlops-questions-answered</td>\n",
       "      <td>2023-05-18 05:21:04.346819 UTC</td>\n",
       "      <td>2023-05-18 01:08:57.948139 UTC</td>\n",
       "      <td>C015J2Y9RLM</td>\n",
       "      <td>I always oppose the counterargument, why do yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>threads</td>\n",
       "      <td>U01J0NVNE1G</td>\n",
       "      <td>mlops-questions-answered</td>\n",
       "      <td>2023-05-18 05:19:51.718379 UTC</td>\n",
       "      <td>2023-05-18 01:08:57.948139 UTC</td>\n",
       "      <td>C015J2Y9RLM</td>\n",
       "      <td>I find MLflow more convenient to use. Here are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>threads</td>\n",
       "      <td>U01CRVDS4NA</td>\n",
       "      <td>mlops-questions-answered</td>\n",
       "      <td>2023-05-18 05:14:52.514189 UTC</td>\n",
       "      <td>2023-05-16 23:22:04.332479 UTC</td>\n",
       "      <td>C015J2Y9RLM</td>\n",
       "      <td>I just built some demos with it. The developer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>messages</td>\n",
       "      <td>U01VCA57PD0</td>\n",
       "      <td>mlops-questions-answered</td>\n",
       "      <td>2023-05-18 01:08:57.948139 UTC</td>\n",
       "      <td>2023-05-18 01:08:57.948139 UTC</td>\n",
       "      <td>C015J2Y9RLM</td>\n",
       "      <td>These days I'm feeling very tempted to roll my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>messages</td>\n",
       "      <td>U015BH45ZK6</td>\n",
       "      <td>mlops-questions-answered</td>\n",
       "      <td>2023-05-17 14:56:59.775629 UTC</td>\n",
       "      <td>2023-05-17 14:56:59.775629 UTC</td>\n",
       "      <td>C015J2Y9RLM</td>\n",
       "      <td>I ran into a problem downloading files from Az...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    __Source      User_ID              Channel_Name  \\\n",
       "3    threads  U01J0NVNE1G  mlops-questions-answered   \n",
       "4    threads  U01J0NVNE1G  mlops-questions-answered   \n",
       "5    threads  U01CRVDS4NA  mlops-questions-answered   \n",
       "7   messages  U01VCA57PD0  mlops-questions-answered   \n",
       "21  messages  U015BH45ZK6  mlops-questions-answered   \n",
       "\n",
       "                 Message_Timestamp                 Thread_Timstamp  \\\n",
       "3   2023-05-18 05:21:04.346819 UTC  2023-05-18 01:08:57.948139 UTC   \n",
       "4   2023-05-18 05:19:51.718379 UTC  2023-05-18 01:08:57.948139 UTC   \n",
       "5   2023-05-18 05:14:52.514189 UTC  2023-05-16 23:22:04.332479 UTC   \n",
       "7   2023-05-18 01:08:57.948139 UTC  2023-05-18 01:08:57.948139 UTC   \n",
       "21  2023-05-17 14:56:59.775629 UTC  2023-05-17 14:56:59.775629 UTC   \n",
       "\n",
       "     Channel_ID                                             __Text  \n",
       "3   C015J2Y9RLM  I always oppose the counterargument, why do yo...  \n",
       "4   C015J2Y9RLM  I find MLflow more convenient to use. Here are...  \n",
       "5   C015J2Y9RLM  I just built some demos with it. The developer...  \n",
       "7   C015J2Y9RLM  These days I'm feeling very tempted to roll my...  \n",
       "21  C015J2Y9RLM  I ran into a problem downloading files from Az...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all data for specific channel\n",
    "df_mlops_questions_answered = messages_df[messages_df[\"Channel_Name\"] == \"mlops-questions-answered\"]\n",
    "df_mlops_questions_answered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages: 20450\n"
     ]
    }
   ],
   "source": [
    "# Get the count of messages in that channel\n",
    "print(f\"Number of messages: {len(df_mlops_questions_answered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That's a lot of messages. \n",
    "\n",
    "As you know from our Slack, that conversations are when you combine messages from one thread. Let's see how many threads or conversations we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1686708556960,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "NhceGvxtOUff",
    "outputId": "32ba52fe-52fe-4693-c72c-b483a4a6926f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversations: 2086\n"
     ]
    }
   ],
   "source": [
    "# Get the count of conversations\n",
    "print(f\"Number of conversations: {len(df_mlops_questions_answered.groupby(['Thread_Timstamp']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to create an embedding for each message and put it in our database, it might not have all the context of the question. So, instead we will only add each conversation into our RAG database. \n",
    "\n",
    "Let's group our messages by channel name and thread timestamp to generate complete conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1686708557142,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "tBg-2ZBvOZ8p",
    "outputId": "72bae821-3f37-47ee-a301-4c9bcd4433a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Channel: mlops-questions-answered / conversation 2020-06-19 20:44:41.0065 UTC </h2><hr/>\n",
       "<table>\n",
       "<tr><td>U011NTHUKEF</td><td>We use a lot of TPOT library. The main advantage is that it does hyperparameter tunning and also deals with preprocessing steps as well... downside is that it works mostly with scikit-learn algorithms. But there is a small hack you can do to make it work with many algorithms... make a new algo class that inherits both from sklearn BaseEstimators and your algo (Catboost, for example, or Pygams)</td></tr><tr><td>U015CHWG25B</td><td>Aha, I’ve heard that it’s really good!</td></tr><tr><td>U0150LZ578X</td><td>For anyone already using other Kubeflow components, Katib is relatively easy to work with <https://github.com/kubeflow/katib>\n",
       "\n",
       "It parallelizes trials across k8s pods and provides a web UI to visualize the hyperparameter space for each training history!</td></tr><tr><td>U015CHWG25B</td><td><@U013CL3GTB3> Thank you! Looks like this is an entire ML platform rather than just a hyperparameter tuning tool :thinking_face: </td></tr><tr><td>U013CL3GTB3</td><td>Polyaxon is another good one! <https://medium.com/polyaxon/polyaxon-0-0-2-23964df6ef7e|https://medium.com/polyaxon/polyaxon-0-0-2-23964df6ef7e></td></tr><tr><td>U015CHWG25B</td><td>At some point we reviewed about 15 hyperparameter tuning tools in order to choose one that answers our needs. We stopped at NNI from Microsoft (<https://github.com/microsoft/Nni|https://github.com/microsoft/Nni>). This tool is designed to run hyperparameter tuning in several parallel jobs. Unlike many other tools, it supports a lot of different algorithms of hyperparameter tuning. It has a decent UI. Plus, it's OSS from Microsoft :) Do you know other tools which answer our criteria?</td></tr><tr><td>U015CHWG25B</td><td>I want to run hyperparameter tuning on several machines in parallel and track the process in Web UI. What is the best tool for that?</td></tr></table>\n",
       "<hr/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Channel: mlops-questions-answered / conversation 2020-06-19 20:45:07.0067 UTC </h2><hr/>\n",
       "<table>\n",
       "<tr><td>U011NTHUKEF</td><td>AirFlow is a great tool. We use it a lot. When we work on a AWS environment, we use AWS stepfunctions which has its own Data Science SDK. Works like a charm!</td></tr><tr><td>U015CHWG25B</td><td>This is a clear usecase for pipeline tools. There are plenty of them out there, providing various features and UI capabilities. Currently, for our projects we use AirFlow. Which pipeline tools do you prefer and why?</td></tr><tr><td>U015CHWG25B</td><td>OK, I have a model of decent quality. Now I want to automate daily collecting data, retraining the model, and redeployment. How do I do that?</td></tr></table>\n",
       "<hr/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Channel: mlops-questions-answered / conversation 2020-06-24 00:53:14.0183 UTC </h2><hr/>\n",
       "<table>\n",
       "<tr><td>U015CHWG25B</td><td>Sure, thank you! :pray:</td></tr><tr><td>U016A3RAL5N</td><td><@U015CHWG25B> thanks for including us in your list. If you need any support lest us know</td></tr></table>\n",
       "<hr/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Channel: mlops-questions-answered / conversation 2020-06-24 17:51:24.026 UTC </h2><hr/>\n",
       "<table>\n",
       "<tr><td>UV92GMLF4</td><td><@U014Z58NT25> Sorry for the late reply:\n",
       "\n",
       "1. It's determined by marketing. The thing that we do it's just to highlight who is in the border. \n",
       "2. We used only numericals in that time, but OHE work fine also. \n",
       "3. At the that time, we didn't used any specific tool. I think today it's doable to do in matplotlib if you transform in 2D array those records from the cluster. </td></tr><tr><td>U012YQULW4X</td><td>you seem to treat it as a technical problem (automate the encoding). This issue points you to a potentially significant change in the input data so you want to notice and manually investigate. For detection, you can check tools for data monitoring like <https://github.com/great-expectations/great_expectations> or tensorflow data monitoring. They should be able to check feature cardinality (number of segments).\n",
       "\n",
       "Segments: Do I understand you correctly that the marketing team changed the logic for the segment variable e.g. an prior \"A\" customer might now be a \"B\" customer?\n",
       "Case A: Feature is not important. Fix encoding bug and move on.\n",
       "Case B: The feature is important but not a lot of customers migrated their segment. Fix encoding bug and move on.\n",
       "Case C:  Feature is important and the logic is very different or many people migrated. This is possibly a breaking change in the data and you need a migration plan (discard training data with old logic, not use the feature for the migration duration etc):\n",
       "\n",
       "you mention: \"everytime this happens\". That should not happen regularly. If it does, you have to stop using the feature/understand better what they do and what it means. Constantly changing data definitions make the feature dangerous to use.</td></tr><tr><td>UP3T8K9M5</td><td>maybe <@U012YQULW4X> can also help as she has some experience with monitoring and will be talking to us about it during this week’s meetup</td></tr><tr><td>U014Z58NT25</td><td>And thanks for the answers guys! It really helps even just to talk about the problem</td></tr><tr><td>U014Z58NT25</td><td><@UV92GMLF4> the clusters were determined by the marketing in the sense of \"good cluster/bad cluster\" or they even gave you a centroid for that? Could you use anything but numericals for KMeans? Essentially I know you can't, for you have to deal with distances. But someone has once told me you can sometimes work with binary/ordinal using KMeans as well.\n",
       "Also, did you have anything (besides your own code and CLI) for monitoring those jumpers? Any specific tools or dashboards</td></tr><tr><td>U014Z58NT25</td><td><@U013CL3GTB3> What happens is that as of now I have, say, 5 customer segments. On the data that I extract tomorrow, I might have 6, cause this customer segmentation is something still being created by the business, and I won't know it in advance.\n",
       "\n",
       "I can extract some meaning from these classes and put them into an ascending order (of integers, for example). And yes, I wanna use them for my recsys at the end of the day.\n",
       "\n",
       "EDIT: typo</td></tr><tr><td>UP3T8K9M5</td><td><@U0156CADGJG> might have something to say about this too</td></tr><tr><td>UV92GMLF4</td><td>Those guys I removed from the clusters and put them in another cluster (determined by marketing).\n",
       "\n",
       "Was totally primitive and I was using the KMeans, but the general idea it's that.</td></tr><tr><td>UV92GMLF4</td><td>In pink I got the \"jumpers\".</td></tr><tr><td>UV92GMLF4</td><td>Those are the clusters (just for simplification)</td></tr><tr><td>UV92GMLF4</td><td>Hey Murilo! What's up!\n",
       "\n",
       "I worked with that in a long time ago (2015) so maybe my answer it's outdated.\n",
       "\n",
       "In my case, I had fixed clusters (due to the fact those clusters where determined by Marketing); so the only thing that I monitor in this case was 1) the Centroid (to check if there's some changes in the centroid (drift) and 2) in the instances that I call \"jumpers\", i.e. records that shifted to some point of clusters boarders.\n",
       "\n",
       "Like this.</td></tr><tr><td>U013CL3GTB3</td><td>• Are these segment features known in advance? Or are they being generated on the fly? Are they different every time? Is it possible to know the values in advance? \n",
       "• can you clarify what you mean by “encoding” ? Are you given a string and want to turn it into a int or float? I’m assuming you use it as a feature for the recommender system.</td></tr><tr><td>U014Z58NT25</td><td>Hey guys! In the matter of concept drifting, I have a clustering algorithm (that is mainly a recommendation system for our business) that takes into account a categorical variable called \"Customer Segment\", that is translated in an ordinal fashion - i.e. \"better\" clients go to a higher number. Due to this new scenario we're going through, some new segments showed up and everytime this happens I have to manually determine how to encode them. Is there a way to automate this? Or even tools to identify when it happens would already help me out a lot. Thanks!</td></tr></table>\n",
       "<hr/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Channel: mlops-questions-answered / conversation 2020-06-27 19:23:54.0496 UTC </h2><hr/>\n",
       "<table>\n",
       "<tr><td>U016FCTSDGS</td><td>Few answers from our experience:\n",
       "1. We keep Airflow stack separate from DAG Projects. We also have a single repo per DAG project that gets auto-named and deployed in such a way that Airflow picks it up to sync in the DAG folder.  We also have a DAG that runs frequently to scan and ingest the new DAG(s) from the DAG Projects.\n",
       "2. If the question is long running tasks we spin up AWS Batch or AWS EMR depending on the need. If the question is how to interject DAG updates if a long running task is executing then no solution here other than on the next run the task(s)/DAG will get updated.\n",
       "3. Did not know about the *`PythonVirtualenvOperator`*  or would have used it. We just installed custom python packages to a temp directory and added to Python path, then deleted the folder in the last task for cleanup.\n",
       "4. Haven't used Docker Operator.\n",
       "5. Let me know if you find one!</td></tr><tr><td>U015CHWG25B</td><td>I can’t answer all your questions one by one, but I can share the basics of our approach.\n",
       "• Regarding deployment: We use Git to deploy DAGs. In our case, we have a dedicated repo for DAGs, and AirFlow installation polls this repo every minute and redeploys DAGs if necessary. Thus your DAGs are separated from your Airflow service.\n",
       "• Regarding tasks: We chose to make our own operator (as we have an ML platform with some specificity) based on DockerOperator. Containerisation of each step gives you way more freedom, especially in complex pipelines when you may need even different versions of CUDA for different steps. \n",
       "</td></tr><tr><td>U013K7876BF</td><td>thanks for the responses <@U013CL3GTB3> and <@U0158N59C8H>\n",
       "Flux sounds great, but I’m going need to work with a push model (we have some constraints in our system -  we can’t access the repo from the cloud)\n",
       "If i’m pushing the scheduler just to kick off a `KubernetesPodOperator`, Airflow feels very similar to Argo.</td></tr><tr><td>UP3T8K9M5</td><td>hmmm yeah i also would like to know this.</td></tr><tr><td>U0158N59C8H</td><td>Nope. I mean I use argo quite a bit via kubeflow pipelines. I used airflow too. Just wanting to get more opinions on the diff solutions.\n",
       "\n",
       "Esp Argo vs Tekton. As they are very closely related.</td></tr><tr><td>UP3T8K9M5</td><td>I also remember <@U0158N59C8H> was asking about Argo a while back. Were you able to get any info that could help us? </td></tr><tr><td>U013CL3GTB3</td><td>1. I would launch the task in a pod using the kubernetesPodOperator. And for pushing your dags, use gitops! You can point your airflow deployment to a git branch. This requires flux and some other stuff to be in place.\n",
       "2. Not sure what you mean by long running airflow service? Are you saying how to update a task independent of airflow itself like the web server or scheduler?  - if you're launching your tasks in pods you leave the execution logic to the docker container running in the pod and the orchestration logic to airflow. I would also recommend using the KubernetesExecutor alongside the KubernetesPodOperator. \n",
       "3. The kubernetesPodOperator also solves this problem as you have complete control over the runtime environment, resources, etc - the container has everything it needs to run the code\n",
       "4. Haven’t used the docker operator but It sounds like it’s more general than just python code. And will allow you to containerize your code which deals with the dependency issue.\n",
       "5. My company BenevolentAI Is about to publish an article about some lessons we learned using airflow so I’ll post it to the community when it’s out.\n",
       "Also me and <@UP3T8K9M5> are planning on doing a coffee session on pipelines - and airflow will be one of them. You mentioned Argo, that’s also another good option as it’s native to kubernetes which is nice and also has other benefits.</td></tr><tr><td>U013K7876BF</td><td>Hey everyone\n",
       "We’re looking for productionize some data engineering pipelines but we really want this run on Airflow and have a proper CI/CD pipeline.\n",
       "I have a couple of questions, and I’m hoping some of you may have experience and knowledge about these topics.\n",
       "1. Would you deploy the Airflow task along with the Airflow service itself? How else would you push your DAGs from source control into the dags folder of Airflow?\n",
       "2. Is there an elegant way to update the tasks separately from a long running Airflow service?\n",
       "3. Given that not all tasks have the same python dependencies, how does dependency management work? Do you install all of the dependencies with Airflow itself, or do you use `PythonVirtualenvOperator` to create a virtual env for each step in the dag?\n",
       "4. How difficult is monitoring and debugging when using `airflow.operators.docker_operator`? we can package each step in our data engineering pipeline in an .py file with a `main` and create a container. Does it have clear benefits over the `PythonVirtualenvOperator?`\n",
       "5. If there are good resource on airflow deployment strategies, I’d love to read more. I was looking at Argo <https://github.com/argoproj/argo> and if help have experience with Argo vs Airflow I’d love to hear that too.\n",
       "Thanks!</td></tr></table>\n",
       "<hr/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate through conversations\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "NUM_CONVERSATIONS_TO_PRINT = 5\n",
    "current_conversation = 0\n",
    "for (channel_name, thread_id), conv in df_mlops_questions_answered.groupby([\"Channel_Name\", \"Thread_Timstamp\"]):\n",
    "    html = f\"<h2>Channel: {channel_name} / conversation {thread_id} </h2><hr/>\\n<table>\\n\"\n",
    "    for index, row in conv.iterrows():\n",
    "        html += f'<tr><td>{row[\"User_ID\"]}</td><td>{row[\"__Text\"]}</td></tr>'\n",
    "    html += \"</table>\\n<hr/><br/>\"\n",
    "    display(HTML(html))\n",
    "    current_conversation += 1\n",
    "    if current_conversation >= NUM_CONVERSATIONS_TO_PRINT:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1W_efjwNiut"
   },
   "source": [
    "### Conversations & Embeddings\n",
    "\n",
    "We've pre-built these conversations to make it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1686708585766,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "xTGDU-5RKwLz",
    "outputId": "87898763-f5c1-42ef-9be8-1fc49c541544"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>chat_text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>africa</td>\n",
       "      <td>2022-03-22 19:42:06.219769 UTC</td>\n",
       "      <td>U024WRAA0D9: Hello fellow MLOpsers in Africa :...</td>\n",
       "      <td>A user named U024WRAA0D9 welcomes fellow MLOps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>africa</td>\n",
       "      <td>2022-03-24 08:14:33.140029 UTC</td>\n",
       "      <td>U024WRAA0D9: What should our next steps be (fo...</td>\n",
       "      <td>The conversation is discussing the next steps ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>africa</td>\n",
       "      <td>2022-03-28 11:57:42.840049 UTC</td>\n",
       "      <td>U024WRAA0D9: What’s everyone’s timezone?U024WR...</td>\n",
       "      <td>The conversation is discussing everyone's time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>africa</td>\n",
       "      <td>2022-04-12 14:36:00.144498 UTC</td>\n",
       "      <td>U03142DQP6Z: Please can we make it later in th...</td>\n",
       "      <td>One person asks if it's possible to schedule a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>africa</td>\n",
       "      <td>2022-04-19 10:24:57.455849 UTC</td>\n",
       "      <td>U024WRAA0D9: Hello &lt;#C037GTG932B|africa&gt;, I on...</td>\n",
       "      <td>Someone named U024WRAA0D9 mentions in the Slac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 channel_name                       thread_id  \\\n",
       "0           0       africa  2022-03-22 19:42:06.219769 UTC   \n",
       "1           1       africa  2022-03-24 08:14:33.140029 UTC   \n",
       "2           2       africa  2022-03-28 11:57:42.840049 UTC   \n",
       "3           3       africa  2022-04-12 14:36:00.144498 UTC   \n",
       "4           4       africa  2022-04-19 10:24:57.455849 UTC   \n",
       "\n",
       "                                           chat_text  \\\n",
       "0  U024WRAA0D9: Hello fellow MLOpsers in Africa :...   \n",
       "1  U024WRAA0D9: What should our next steps be (fo...   \n",
       "2  U024WRAA0D9: What’s everyone’s timezone?U024WR...   \n",
       "3  U03142DQP6Z: Please can we make it later in th...   \n",
       "4  U024WRAA0D9: Hello <#C037GTG932B|africa>, I on...   \n",
       "\n",
       "                                             summary  \n",
       "0  A user named U024WRAA0D9 welcomes fellow MLOps...  \n",
       "1  The conversation is discussing the next steps ...  \n",
       "2  The conversation is discussing everyone's time...  \n",
       "3  One person asks if it's possible to schedule a...  \n",
       "4  Someone named U024WRAA0D9 mentions in the Slac...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each conversation grouped into a single thread_id\n",
    "chats_df = pd.read_csv(\"./.content/chats/chats.csv\")\n",
    "chats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, as we build our vector embeddings, we would use an embedding model. You could use an online model (e.g. form OpenAI, Cohere, etc.) or using a local model (e.g. using T5 or BERT). In this case, since we have about 9.7k conversations, we've pre-embedded these for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2913,
     "status": "ok",
     "timestamp": 1686708588675,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "wHQUy394K_5s",
    "outputId": "65583a01-ef4c-41de-fda2-0a2222837a33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-22 19:42:06.219769 UTC</td>\n",
       "      <td>[0.0036350293084979057, -0.01264416053891182, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-24 08:14:33.140029 UTC</td>\n",
       "      <td>[-0.002360287122428417, -0.04199115186929703, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-03-28 11:57:42.840049 UTC</td>\n",
       "      <td>[0.017543835565447807, 0.0032007887493819, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-04-12 14:36:00.144498 UTC</td>\n",
       "      <td>[-0.001173610333353281, -0.014446504414081573,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-04-19 10:24:57.455849 UTC</td>\n",
       "      <td>[-0.0025763490702956915, -0.02925489842891693,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       thread_id  \\\n",
       "0           0  2022-03-22 19:42:06.219769 UTC   \n",
       "1           1  2022-03-24 08:14:33.140029 UTC   \n",
       "2           2  2022-03-28 11:57:42.840049 UTC   \n",
       "3           3  2022-04-12 14:36:00.144498 UTC   \n",
       "4           4  2022-04-19 10:24:57.455849 UTC   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0036350293084979057, -0.01264416053891182, ...  \n",
       "1  [-0.002360287122428417, -0.04199115186929703, ...  \n",
       "2  [0.017543835565447807, 0.0032007887493819, 0.0...  \n",
       "3  [-0.001173610333353281, -0.014446504414081573,...  \n",
       "4  [-0.0025763490702956915, -0.02925489842891693,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The embedding for each conversation with its thread_id (Note: not all embeddings were generated for the chat text)\n",
    "embeddings_df = pd.read_csv(\"./.content/chats/chats-embeddings-ada-002.csv\")\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deserialize the string embeddings into vectors. (The API you use for your embeddings should automatically return you a list of floats. this step is only because we prebuilt the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all conversations:  9719\n",
      "Number of conversations with embeddings:  9713\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of conversations and embeddings do not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/rahulparundekar/workspaces/ai-hero/course-intro-to-qa-systems-with-llms/poc/explore.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rahulparundekar/workspaces/ai-hero/course-intro-to-qa-systems-with-llms/poc/explore.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of all conversations: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(chats_df))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rahulparundekar/workspaces/ai-hero/course-intro-to-qa-systems-with-llms/poc/explore.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of conversations with embeddings: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(embeddings_df))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rahulparundekar/workspaces/ai-hero/course-intro-to-qa-systems-with-llms/poc/explore.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(chats_df) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(embeddings_df), \u001b[39m\"\u001b[39m\u001b[39mNumber of conversations and embeddings do not match\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of conversations and embeddings do not match"
     ]
    }
   ],
   "source": [
    "print(\"Number of all conversations: \", len(chats_df))\n",
    "print(\"Number of conversations with embeddings: \", len(embeddings_df))\n",
    "# NOTE: We have some missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 10972,
     "status": "ok",
     "timestamp": 1686708599641,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "4wAmWK4dMg_8",
    "outputId": "62eb1a22-e7df-4b9e-d10d-6dfd56ffa43f"
   },
   "outputs": [],
   "source": [
    "# Get your embeddings data together.\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a temp index of the chats\n",
    "chats_index = {}\n",
    "for _, row in tqdm(chats_df.iterrows(), desc=\"Creating temporary chats index\"):\n",
    "    chats_index[row[\"thread_id\"]] = row[\"chat_text\"]\n",
    "\n",
    "# Link the chats and embeddings together\n",
    "embeddings = []\n",
    "VECTOR_SIZE = None\n",
    "for index, row in tqdm(embeddings_df.iterrows(), desc=\"Collecting chats and embeddings\"):\n",
    "    chat_row = chats_df.iloc[index]\n",
    "    embedding = json.loads(row[\"embedding\"])\n",
    "    embeddings.append({\"thread_id\": row[\"thread_id\"], \"text\": chat_row[\"chat_text\"], \"embedding\": embedding})\n",
    "    if not VECTOR_SIZE:\n",
    "        VECTOR_SIZE = len(embedding)\n",
    "    else:\n",
    "        assert VECTOR_SIZE == len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_MIq52NMbyT"
   },
   "source": [
    "# Create your vector database\n",
    "Feel free to choose the Vector DB you want to use, but for the tutorial we will be using an open-source DB - ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1686708599642,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "IKHwaN-MLICJ",
    "outputId": "bdab29a1-52fb-451b-dc11-ac28a984e706"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection(\"chats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1686708605689,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "dtAsNPAEL-fV",
    "outputId": "8baab083-6e51-495c-d56b-cc4ce0046e7c"
   },
   "outputs": [],
   "source": [
    "embeddings_list = []\n",
    "metadata_list = []\n",
    "id_list = []\n",
    "\n",
    "for row in tqdm(embeddings):\n",
    "    metadata = {\n",
    "        \"thread_id\": row[\"thread_id\"],\n",
    "        \"text\": chats_index[row[\"thread_id\"]],\n",
    "    }\n",
    "    metadata_list.append(metadata)\n",
    "    embeddings_list.append(row[\"embedding\"])\n",
    "    id_list.append(row[\"thread_id\"])\n",
    "\n",
    "collection.add(embeddings=embeddings_list, metadatas=metadata_list, ids=id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  check the data in the vector db and confirm the step above was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4VCtBrfOXT_"
   },
   "outputs": [],
   "source": [
    "print(\"Number of conversations in vector DB: \", collection.count())  # returns the number of items in the collection\n",
    "collection.peek(1)  # returns a list of the first n items in the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a function for searching for the closest chats given an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1686709312554,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "pS_lC8k6SJTQ",
    "outputId": "a5b59dc1-4da2-4556-b15b-da756dbaefaa"
   },
   "outputs": [],
   "source": [
    "def search_index(embedding):\n",
    "    results = collection.query(query_embeddings=[embedding], n_results=3)\n",
    "    return [{\"thread_id\": m[\"thread_id\"], \"text\": m[\"text\"]} for m in results[\"metadatas\"][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sanity check, test with an existing document, that that document is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1686709471504,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "jI_XJYqwY0pP",
    "outputId": "3c37dbbe-e163-4143-ff8f-c53eaaf450a7"
   },
   "outputs": [],
   "source": [
    "row = embeddings[2539]\n",
    "docs = search_index(row[\"embedding\"])\n",
    "assert row[\"thread_id\"] == docs[0][\"thread_id\"], \"Document does not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuiB3wZHbb2_"
   },
   "source": [
    "# Example Q&A with OpenAI\n",
    "\n",
    "Now that we have our vector database, let's create the Q&A mechanism. It'll work like this:\n",
    "\n",
    "For a question that the user asks:\n",
    "- Let's generate the embedding for that using the same embedding model as our documents (i.e. OpenAI's Ada-002)\n",
    "- Query the database to get the 3 nearest documents. \n",
    "- Since these documents would be too large for the context window of openai, we need to summarize them.\n",
    "- Finally, we generate the answer for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "executionInfo": {
     "elapsed": 10191,
     "status": "ok",
     "timestamp": 1686709489625,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "GRTZ6vB1ZfB6",
    "outputId": "00712dec-d760-4bf2-818b-bf690114a35e"
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1686709578912,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "Ij-IEfqlbevd",
    "outputId": "c0aeb548-e594-49fe-a397-26dd15e53f4f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Please save your OPENAI_API_KEY in a .env file.\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    # Use the same embedding generator as what was used on the data!!!\n",
    "    response = openai.Embedding.create(model=\"text-embedding-ada-002\", input=text)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def summarize(chat_text):\n",
    "    # Summarize conversations since individually they are long and go over 8k limit\n",
    "    prompt = (\n",
    "        \"Summarize the following conversation on the MLOps.community slack channel. Do not use the usernames in the summary. ```\"\n",
    "        + chat_text\n",
    "        + \"```\"\n",
    "    )\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def extract_answer(chat_texts, question):\n",
    "    # Combine the summaries into a prompt and use SotA GPT-4 to answer.\n",
    "    prompt = \"Use the following summaries of conversations on the MLOps.community slack channel backtics to generate an answer for the user question.\"\n",
    "    for i, chat_text in enumerate(chat_texts):\n",
    "        print(f\"Getting summary for conversation {i+1}\")\n",
    "        prompt += f\"\\nConversation {i+1} Summary:\\n```\\n{summarize(chat_text)}```\"\n",
    "\n",
    "    if not question.endswith(\"?\"):\n",
    "        question = question + \"?\"\n",
    "    prompt += f\"\\nQuestion: {question}\"\n",
    "    print(\"Getting answer for the question.\")\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    content = completion.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1686709638802,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "4FTUR7aSbiR4",
    "outputId": "73415edf-1706-4fdf-8af4-7b67a922cb93"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_answer(question):\n",
    "    tic = time.perf_counter()\n",
    "    # Get answer to the question by finding the three conversations that are nearest\n",
    "    # to the question and then using them to generate the answer.\n",
    "    print(\"Searching documents nearest to the question.\")\n",
    "    search_vector = get_embedding(question)\n",
    "    docs = search_index(search_vector)\n",
    "    # Take the top three answers, and use ChatGPT to form the answer to give the user.\n",
    "    chat_texts = []\n",
    "    for doc in docs:\n",
    "        chat_text = chats_index[doc[\"thread_id\"]]\n",
    "        chat_texts.append(chat_text)\n",
    "    if len(chat_texts) > 3:\n",
    "        chat_texts[:3]\n",
    "    answer = extract_answer(chat_texts, question)\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Answer took {toc - tic:0.4f} seconds\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "executionInfo": {
     "elapsed": 32320,
     "status": "ok",
     "timestamp": 1686709686394,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "bZDd3d5lcSSz",
    "outputId": "cff3b60f-37ea-401e-81e1-a090664c512a"
   },
   "outputs": [],
   "source": [
    "question = \"What are some good ways to deploy models on Kubernetes?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"\\n\\nQuestion: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "executionInfo": {
     "elapsed": 58520,
     "status": "ok",
     "timestamp": 1686709746226,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "Ub0bI4roeUHK",
    "outputId": "4f01f8e7-d316-428c-ed53-041c47a67769"
   },
   "outputs": [],
   "source": [
    "question = \"How can I structure a good Data Science team?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"\\n\\nQuestion: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 47208,
     "status": "ok",
     "timestamp": 1686709801666,
     "user": {
      "displayName": "Rahul Parundekar",
      "userId": "17922132101761181839"
     },
     "user_tz": 420
    },
    "id": "Hx4hElL0kzlV",
    "outputId": "4ca0ff52-d2fa-45da-8030-38e035cd9178"
   },
   "outputs": [],
   "source": [
    "question = \"What is the best way to train models for tabular data?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"\\n\\nQuestion: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Feel free to play around with this proof of concept. \n",
    "\n",
    "We also have companion code with some optimizations for you when you're ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observtions\n",
    "\n",
    "- We note that even though we need the UI to look like a Q&A system.\n",
    "- Also, another observation is that while answering, the code makes 4 calls to OpenAI - three to summarize and one to generate the response. \n",
    "\n",
    "So we optimize our code for the above and create the PoC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNfGgXP5MpHq2EVyOLNt4Ca",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
